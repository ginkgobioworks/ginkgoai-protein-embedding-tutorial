{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train something\n",
    "\n",
    "Now that we have our data, let's train a model. \n",
    "\n",
    "But before we train anything, we should discuss what kind of model we are training. Most typical ML problems fall into one of two categories: classification and regression. Classification models try to predict a class for each datum; for example, what dog breed is present in an image. Regression tasks try to predict a number. Our problem is a classical regression task: given a sequence, the model tries to predict the temperature.\n",
    "\n",
    "Machine learning models are trained based on how far they are from the desired value. This is called a `loss` metric, and it's one of the most important aspects of training. A typical loss metric in regression tasks is the root mean square deviation ([RMSD](https://en.wikipedia.org/wiki/Root_mean_square_deviation)).\n",
    "\n",
    "Another important concept is performance metric - how do we determine if our model is better or worse. For regression problems, a good candidate is the [Pearson correlation coefficient](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html#pearsonr) (otherwise known as Pearson's R, related to the coefficient of determination $R^2$). Let's use this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MKGIRARLTANFMIIIIITVTILEVLLIYTVRQNYYGSLEGSLTNQ...</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MGGVGVLTLMVGVRVSPEPAVLGLLERYRDALNYSIRVLIESKTIS...</td>\n",
       "      <td>77.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAKKKDTPGDGEFPGFSDTLQRTPKLEKPHYAGHRDRLKQRFRDAP...</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNFGDKVRYVRKKLSLSTEQLAKLLDVTQSYISHIENNRRLLGRDK...</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MKNDINIKNKRAYFDYNLLDKYVAGIALLGTEIKAIRQGKANMTDA...</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  temperature\n",
       "0  MKGIRARLTANFMIIIIITVTILEVLLIYTVRQNYYGSLEGSLTNQ...    28.000000\n",
       "1  MGGVGVLTLMVGVRVSPEPAVLGLLERYRDALNYSIRVLIESKTIS...    77.333333\n",
       "2  MAKKKDTPGDGEFPGFSDTLQRTPKLEKPHYAGHRDRLKQRFRDAP...    30.000000\n",
       "3  MNFGDKVRYVRKKLSLSTEQLAKLLDVTQSYISHIENNRRLLGRDK...    35.000000\n",
       "4  MKNDINIKNKRAYFDYNLLDKYVAGIALLGTEIKAIRQGKANMTDA...    18.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv('../data/test_df.csv')\n",
    "train_df = pd.read_csv('../data/train_df.csv')\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use an [XGBoost](https://xgboost.readthedocs.io/en/latest/index.html) model. It's a very powerful and useful model architecture. It excels at tabular data and is pretty foolproof on its own (assuming your dataset is well prepared). It doesn't require any GPUs and can handle thousands of columns in a dataset, which will become important in the next notebook.\n",
    "\n",
    "Let's start with random inputs and attempt to predict the result. This will tell us what \"the floor\" is. It's quite common for a model to find a way to \"cheat\" the predictions. For example, in our case (based on the distributions of temperatures we explored in the previous notebook), the model could just predict `36` and it would give pretty good results. We avoided this outcome by balancing our dataset, but such patterns are always something to keep an eye on.\n",
    "\n",
    "Training on random data will show what score such a \"cheat\" would get. Our goal is to meaningfully improve on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create random features as input (same number of rows as training data)\n",
    "n_features = 200  # arbitrary number of random features\n",
    "random_features = np.random.normal(size=(train_df.shape[0], n_features))\n",
    "random_feature_names = [f'random_{i}' for i in range(n_features)]\n",
    "\n",
    "random_df = pd.DataFrame(random_features, columns=random_feature_names)\n",
    "random_df[\"temperature\"] = train_df[\"temperature\"]\n",
    "\n",
    "random_df_train, random_df_test = train_test_split(random_df, shuffle=False, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model on our random data. As you'll see, XGBoost is really fast to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.011074818505570464\n",
      "Test MSE: 992.6919691817417\n",
      "Train R2: 0.9999857180017291\n",
      "Test R2: -0.15596125491738877\n",
      "\n",
      "Pearson correlation on test set: 0.0057\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Specify model hyperparameters\n",
    "model = XGBRegressor(\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train = random_df_train[random_feature_names]\n",
    "y_train = random_df_train['temperature']\n",
    "X_test = random_df_test[random_feature_names]\n",
    "y_test = random_df_test['temperature']\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "train_pred = model.predict(X_train) \n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, train_pred))  \n",
    "print(\"Test MSE:\", mean_squared_error(y_test, test_pred))\n",
    "print(\"Train R2:\", r2_score(y_train, train_pred))\n",
    "print(\"Test R2:\", r2_score(y_test, test_pred))\n",
    "\n",
    "# Calculate Pearson correlation\n",
    "pearson_corr = np.corrcoef(y_test, test_pred)[0,1]\n",
    "print(f\"\\nPearson correlation on the test set: {pearson_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our correlation with random data on our held-out validation set is almost `0`. That's a good sign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on some data\n",
    "\n",
    "Next, let's go for the simplest approach to training: using integer representation of amino acids. This is similar to the original dataset representation, so we will reverse our processing in a way. It's unlikely that the model will perform well here as well, but it will show us if there are any further issues with our dataset. An example could be that proteins that start with `M` have higher temperature stability, which doesn't make a lot of sense on its own, but could be a sign of an imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa_1</th>\n",
       "      <th>aa_2</th>\n",
       "      <th>aa_3</th>\n",
       "      <th>aa_4</th>\n",
       "      <th>aa_5</th>\n",
       "      <th>aa_6</th>\n",
       "      <th>aa_7</th>\n",
       "      <th>aa_8</th>\n",
       "      <th>aa_9</th>\n",
       "      <th>aa_10</th>\n",
       "      <th>...</th>\n",
       "      <th>aa_642</th>\n",
       "      <th>aa_643</th>\n",
       "      <th>aa_644</th>\n",
       "      <th>aa_645</th>\n",
       "      <th>aa_646</th>\n",
       "      <th>aa_647</th>\n",
       "      <th>aa_648</th>\n",
       "      <th>aa_649</th>\n",
       "      <th>aa_650</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77</td>\n",
       "      <td>75</td>\n",
       "      <td>71</td>\n",
       "      <td>73</td>\n",
       "      <td>82</td>\n",
       "      <td>65</td>\n",
       "      <td>82</td>\n",
       "      <td>76</td>\n",
       "      <td>84</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>86</td>\n",
       "      <td>71</td>\n",
       "      <td>86</td>\n",
       "      <td>76</td>\n",
       "      <td>84</td>\n",
       "      <td>76</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>77.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77</td>\n",
       "      <td>65</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>68</td>\n",
       "      <td>84</td>\n",
       "      <td>80</td>\n",
       "      <td>71</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>70</td>\n",
       "      <td>71</td>\n",
       "      <td>68</td>\n",
       "      <td>75</td>\n",
       "      <td>86</td>\n",
       "      <td>82</td>\n",
       "      <td>89</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77</td>\n",
       "      <td>75</td>\n",
       "      <td>78</td>\n",
       "      <td>68</td>\n",
       "      <td>73</td>\n",
       "      <td>78</td>\n",
       "      <td>73</td>\n",
       "      <td>75</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>77</td>\n",
       "      <td>73</td>\n",
       "      <td>72</td>\n",
       "      <td>75</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>69</td>\n",
       "      <td>82</td>\n",
       "      <td>73</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>77</td>\n",
       "      <td>83</td>\n",
       "      <td>84</td>\n",
       "      <td>76</td>\n",
       "      <td>73</td>\n",
       "      <td>70</td>\n",
       "      <td>71</td>\n",
       "      <td>72</td>\n",
       "      <td>75</td>\n",
       "      <td>78</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>77</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>71</td>\n",
       "      <td>86</td>\n",
       "      <td>76</td>\n",
       "      <td>77</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>77</td>\n",
       "      <td>83</td>\n",
       "      <td>69</td>\n",
       "      <td>80</td>\n",
       "      <td>73</td>\n",
       "      <td>80</td>\n",
       "      <td>65</td>\n",
       "      <td>80</td>\n",
       "      <td>65</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>77</td>\n",
       "      <td>75</td>\n",
       "      <td>83</td>\n",
       "      <td>75</td>\n",
       "      <td>86</td>\n",
       "      <td>65</td>\n",
       "      <td>69</td>\n",
       "      <td>82</td>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>85.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1520 rows × 651 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aa_1  aa_2  aa_3  aa_4  aa_5  aa_6  aa_7  aa_8  aa_9  aa_10  ...  \\\n",
       "0       77    75    71    73    82    65    82    76    84     65  ...   \n",
       "1       77    71    71    86    71    86    76    84    76     77  ...   \n",
       "2       77    65    75    75    75    68    84    80    71     68  ...   \n",
       "3       77    78    70    71    68    75    86    82    89     86  ...   \n",
       "4       77    75    78    68    73    78    73    75    78     75  ...   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...    ...  ...   \n",
       "1515    77    73    72    75    82    82    82    69    82     73  ...   \n",
       "1516    77    83    84    76    73    70    71    72    75     78  ...   \n",
       "1517    77    75    76    73    73    71    86    76    77     84  ...   \n",
       "1518    77    83    69    80    73    80    65    80    65     84  ...   \n",
       "1519    77    75    83    75    86    65    69    82    69     70  ...   \n",
       "\n",
       "      aa_642  aa_643  aa_644  aa_645  aa_646  aa_647  aa_648  aa_649  aa_650  \\\n",
       "0         45      45      45      45      45      45      45      45      45   \n",
       "1         45      45      45      45      45      45      45      45      45   \n",
       "2         45      45      45      45      45      45      45      45      45   \n",
       "3         45      45      45      45      45      45      45      45      45   \n",
       "4         45      45      45      45      45      45      45      45      45   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "1515      45      45      45      45      45      45      45      45      45   \n",
       "1516      45      45      45      45      45      45      45      45      45   \n",
       "1517      45      45      45      45      45      45      45      45      45   \n",
       "1518      45      45      45      45      45      45      45      45      45   \n",
       "1519      45      45      45      45      45      45      45      45      45   \n",
       "\n",
       "      temperature  \n",
       "0       28.000000  \n",
       "1       77.333333  \n",
       "2       30.000000  \n",
       "3       35.000000  \n",
       "4       18.000000  \n",
       "...           ...  \n",
       "1515    75.000000  \n",
       "1516    30.000000  \n",
       "1517     9.500000  \n",
       "1518    45.000000  \n",
       "1519    85.000000  \n",
       "\n",
       "[1520 rows x 651 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert sequences to integer encoding\n",
    "def sequence_to_integers(sequence, max_length=650):\n",
    "    # Pad or truncate sequence to max_length\n",
    "    sequence = sequence[:max_length].ljust(max_length, '-')\n",
    "    # Convert to list of integers (using ord() for simple integer encoding)\n",
    "    return [ord(aa) for aa in sequence]\n",
    "\n",
    "# Apply encoding to train sequences\n",
    "train_sequences = train_df['sequence'].apply(sequence_to_integers)\n",
    "# Convert to DataFrame with columns aa_1, aa_2, etc\n",
    "train_encoded = pd.DataFrame(train_sequences.tolist(), \n",
    "                           columns=[f'aa_{i+1}' for i in range(650)])\n",
    "# Add temperature column back\n",
    "train_encoded['temperature'] = train_df['temperature']\n",
    "\n",
    "# Repeat for test set\n",
    "test_sequences = test_df['sequence'].apply(sequence_to_integers)\n",
    "test_encoded = pd.DataFrame(test_sequences.tolist(),\n",
    "                          columns=[f'aa_{i+1}' for i in range(650)])\n",
    "test_encoded['temperature'] = test_df['temperature']\n",
    "\n",
    "# Get feature names (all columns except temperature)\n",
    "feature_names = [col for col in train_encoded.columns if col != 'temperature']\n",
    "\n",
    "train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.022066787655929985\n",
      "Test MSE: 842.4654707183843\n",
      "Train R2: 0.9999721433627133\n",
      "Test R2: -0.07565621068268258\n",
      "\n",
      "Pearson correlation on test set: 0.1607\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "X_train = train_encoded[feature_names]\n",
    "y_train = train_encoded['temperature']\n",
    "X_test = test_encoded[feature_names]\n",
    "y_test = test_encoded['temperature']\n",
    "\n",
    "model = XGBRegressor(\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "train_pred = model.predict(X_train) \n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, train_pred))  \n",
    "print(\"Test MSE:\", mean_squared_error(y_test, test_pred))\n",
    "print(\"Train R2:\", r2_score(y_train, train_pred))\n",
    "print(\"Test R2:\", r2_score(y_test, test_pred))\n",
    "\n",
    "# Calculate Pearson correlation\n",
    "pearson_corr = np.corrcoef(y_test, test_pred)[0,1]\n",
    "print(f\"\\nPearson correlation on the test set: {pearson_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the need for a train/test split, let's show the Pearson correlation of our model on this somewhat nonsensical dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Pearson correlation: 0.9999885866632156\n",
      "Test Pearson correlation: 0.16068236013273213\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "print(f\"Train Pearson correlation: {np.corrcoef(y_train, train_pred)[0,1]}\")\n",
    "print(f\"Test Pearson correlation: {np.corrcoef(y_test, test_pred)[0,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model almost perfectly predicts the training set but completely fails on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, just amino acid integers don't do much better than random. This is expected and good news. In the next part, we'll train an actual working model using deep learning embeddings pushed to XGBoost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
